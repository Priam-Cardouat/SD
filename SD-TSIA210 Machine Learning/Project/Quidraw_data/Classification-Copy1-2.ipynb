{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523ef214",
   "metadata": {},
   "source": [
    "# SD-TSIA210 - binary classification task\n",
    "\n",
    "### Doriand Petit, Priam Cardouat, David GÃ©rard\n",
    "\n",
    "\n",
    "In this part of the project, we are given a database (already pre-processed) consisting of images of simple drawings, that is to say drawings of ants and grapes. We have a binary classification task and will implement different models to carry out this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64f08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "from time import time\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cc163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "ant = np.load('G_20_ant.npy')\n",
    "grapes = np.load('G_20_grapes.npy')\n",
    "print(grapes.shape)\n",
    "X = np.vstack((ant, grapes))\n",
    "y = [0]*len(ant)+[1]*len(grapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fcb1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from scikit-learn\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]), \n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    \n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "  \n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73eca0",
   "metadata": {},
   "source": [
    "One image is composed of 28 x 28 pixels and is converted in a list of 784 elements, and for each object, there are 1000 drawings. The ant drawings will be labeled as 0 and the grapes'one will be labeled as 1 (hence using a binary classification loss). Here is for example the first drawing of the drawings of ants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92878ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "title = \"grapes\"           \n",
    "if y[0]==0:\n",
    "    title = \"ant\"\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc07bb7",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "In this part, we propose to use SVM in order to determine a classifier between our 2 classes.\n",
    "\n",
    "Firstly, we split our data into a training and testing set, keeping 25% of the data for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e28d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2683c36",
   "metadata": {},
   "source": [
    "We will have to find the best choice for the kernel, as well as for the choices of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c722b",
   "metadata": {},
   "source": [
    "### Choices of best parameters and kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be0b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       256\n",
      "           1       0.87      0.91      0.89       244\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.89      0.89      0.89       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7d56305502ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ants\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"grapes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = {'C': [0.1, 1, 10, 100],   \n",
    "              'gamma':['scale', 'auto'],\n",
    "              'degree':[2,3,4],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print(classification_report(Y_test, prediction))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(Y_test, prediction), classes=[\"ants\",\"grapes\"],normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa120a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9e999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"test score\", clf.score(X_test, Y_test))\n",
    "svm_accuracy = clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee913ce0",
   "metadata": {},
   "source": [
    "Hence, the best SVM classifier is given for a gaussian kernel, and the hyperparameters $C=10$, $degree=2$ and gamma is scale. The testing score is in that case equal to 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792074c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5, 5, i)\n",
    "    tpm = np.random.randint(0,len(X_test))\n",
    "    plt.imshow(X_test[tpm].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "    # Prediction\n",
    "    correct = True\n",
    "    if prediction[tpm] != Y_test[tpm]:\n",
    "        correct = False\n",
    "    if prediction[tpm]==0:\n",
    "        label_predicted = \"ant\"\n",
    "    else:\n",
    "        label_predicted = \"grape\"\n",
    "    plt.title(\"Prediction : \" + label_predicted + \", \" + str(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b707d",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Now we will build a binary decision tree. The challenge here is to find the best sepration/split so the impurity criterion and the stopping criterion (maximum depth or number of minimal number of data reached at a node, here we choose the stoping criterion to be a maximum depth reached).\n",
    "For the impurity criterion, we will decide between GINI index and cross-entropy. Regarding the sopping criterion, we will use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f389e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c8253",
   "metadata": {},
   "source": [
    "### GINI index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ee229",
   "metadata": {},
   "source": [
    "First, let us see the tree for the GINI index and for the maximum depth that can be reached until all leaves are pure or until all leaves contain less than 2 samples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion = 'gini')\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\"\"\"\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=[\"pixel number \"+str(k) for k in range(784)],  \n",
    "                         class_names=[\"ANT\", \"GRAPES\"],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graphv = graphviz.Source(dot_data)\n",
    "graphv\"\"\"\n",
    "plt.figure(figsize=(250,100))\n",
    "a = tree.plot_tree(clf,\n",
    "               feature_names = [\"pixel number \"+str(k) for k in range(784)], \n",
    "               class_names=[\"ANT\", \"GRAPES\"],\n",
    "               filled = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.tree_.max_depth)\n",
    "print(\"training score\", clf.score(X_train, Y_train))\n",
    "print(\"test score\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f66f66",
   "metadata": {},
   "source": [
    "We can see that there is overfitting, so we will look for the best depth thanks to cross-validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ef4ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "plt.close(\"all\")\n",
    "# function for fitting trees of various depths on the training data using cross-validation\n",
    "def run_cross_validation_on_trees(X, y, tree_depths, cv=5, scoring='accuracy'):\n",
    "    cv_scores_list = []\n",
    "    cv_scores_std = []\n",
    "    cv_scores_mean = []\n",
    "    accuracy_scores = []\n",
    "    for depth in tree_depths:\n",
    "        tree_model = tree.DecisionTreeClassifier(criterion='gini', max_depth=depth)\n",
    "        cv_scores = cross_val_score(tree_model, X, y, cv=cv, scoring=scoring)\n",
    "        cv_scores_list.append(cv_scores)\n",
    "        cv_scores_mean.append(cv_scores.mean())\n",
    "        cv_scores_std.append(cv_scores.std())\n",
    "        accuracy_scores.append(tree_model.fit(X, y).score(X, y))\n",
    "    cv_scores_mean = np.array(cv_scores_mean)\n",
    "    cv_scores_std = np.array(cv_scores_std)\n",
    "    accuracy_scores = np.array(accuracy_scores)\n",
    "    return cv_scores_mean, cv_scores_std, accuracy_scores\n",
    "  \n",
    "# function for plotting cross-validation results\n",
    "def plot_cross_validation_on_trees(depths, cv_scores_mean, cv_scores_std, accuracy_scores, title):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
    "    ax.plot(depths, cv_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
    "    ax.fill_between(depths, cv_scores_mean-2*cv_scores_std, cv_scores_mean+2*cv_scores_std, alpha=0.2)\n",
    "    ylim = plt.ylim()\n",
    "    ax.plot(depths, accuracy_scores, '-*', label='train accuracy', alpha=0.9)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Tree depth', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xticks(depths)\n",
    "    ax.legend()\n",
    "\n",
    "# fitting trees of depth 1 to 24\n",
    "sm_tree_depths = range(1,25)\n",
    "sm_cv_scores_mean, sm_cv_scores_std, sm_accuracy_scores = run_cross_validation_on_trees(X_train, Y_train, sm_tree_depths)\n",
    "\n",
    "# plotting accuracy\n",
    "plot_cross_validation_on_trees(sm_tree_depths, sm_cv_scores_mean, sm_cv_scores_std, sm_accuracy_scores, \n",
    "                               'Accuracy per decision tree depth on training data')\n",
    "\n",
    "idx_max = sm_cv_scores_mean.argmax()\n",
    "sm_best_tree_depth = sm_tree_depths[idx_max]\n",
    "sm_best_tree_cv_score = sm_cv_scores_mean[idx_max]\n",
    "sm_best_tree_cv_score_std = sm_cv_scores_std[idx_max]\n",
    "print('The depth-{} tree achieves the best mean cross-validation accuracy {} +/- {}% on training dataset'.format(\n",
    "      sm_best_tree_depth, round(sm_best_tree_cv_score*100,5), round(sm_best_tree_cv_score_std*100, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2bcb5",
   "metadata": {},
   "source": [
    "Hence, the optimal depth is a depth of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "clf = tree.DecisionTreeClassifier(criterion = 'gini', max_depth = 4)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "\"\"\"dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=[\"pixel number \"+str(k) for k in range(784)],  \n",
    "                         class_names=[\"ANT\", \"GRAPES\"],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graphv = graphviz.Source(dot_data)\n",
    "graphv\"\"\"\n",
    "plt.figure(figsize=(250,100))\n",
    "a = tree.plot_tree(clf,\n",
    "               feature_names = [\"pixel number \"+str(k) for k in range(784)], \n",
    "               class_names=[\"ANT\", \"GRAPES\"],\n",
    "               filled = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "features = []\n",
    "index = []\n",
    "for i,n in enumerate(clf.feature_importances_):\n",
    "    if n!=0:\n",
    "        features.append(n)\n",
    "        index.append(str(i))\n",
    "plt.close(\"all\")\n",
    "plt.bar(index, features)\n",
    "plt.xticks(range(len(index)), index)\n",
    "plt.ylabel(\"feature importance\")\n",
    "plt.title(\"Feature importances over the decision tree\")\n",
    "plt.show()\n",
    "print(\"training score \", clf.score(X_train, Y_train))\n",
    "print(\"testing score \", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0a67d",
   "metadata": {},
   "source": [
    "### Choice of the best impurity criterion and stopping criterion\n",
    "\n",
    "As previously, we have to find the best hyperparameters for this method as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'criterion':['gini', 'entropy'], 'max_depth':range(1,24)}\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(), parameters)\n",
    "clf.fit(X_train, Y_train)\n",
    "tree_model = clf.best_estimator_\n",
    "print (\"Mean cross-validated score of the best_estimator\", clf.best_score_, \"best parameters \", clf.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814de274",
   "metadata": {},
   "source": [
    "The best decision tree is given below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eed6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\"\"\"dot_data = tree.export_graphviz(tree_model, out_file=None, \n",
    "                         feature_names=[\"pixel number \"+str(k) for k in range(784)],  \n",
    "                         class_names=[\"ANT\", \"GRAPES\"],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graphv = graphviz.Source(dot_data)\n",
    "graphv\"\"\"\n",
    "plt.figure(figsize=(250,100))\n",
    "a = tree.plot_tree(tree_model,\n",
    "               feature_names = [\"pixel number \"+str(k) for k in range(784)], \n",
    "               class_names=[\"ANT\", \"GRAPES\"],\n",
    "               filled = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f506a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score\", tree_model.score(X_train, Y_train))\n",
    "print(\"test score\", tree_model.score(X_test, Y_test))\n",
    "simple_decision_tree_score = tree_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "features = []\n",
    "index = []\n",
    "for i,n in enumerate(tree_model.feature_importances_):\n",
    "    if n!=0:\n",
    "        features.append(n)\n",
    "        index.append(str(i))\n",
    "plt.close(\"all\")\n",
    "plt.bar(index, features)\n",
    "plt.xticks(range(len(index)), index)\n",
    "plt.ylabel(\"feature importance\")\n",
    "plt.title(\"Feature importances over the decision tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "prediction = clf.predict(X_test)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5, 5, i)\n",
    "    tpm = np.random.randint(0,len(X_test))\n",
    "    plt.imshow(X_test[tpm].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "    # Prediction\n",
    "    correct = True\n",
    "    if prediction[tpm] != Y_test[tpm]:\n",
    "        correct = False\n",
    "    if prediction[tpm]==0:\n",
    "        label_predicted = \"ant\"\n",
    "    else:\n",
    "        label_predicted = \"grape\"\n",
    "    plt.title(\"Prediction : \" + label_predicted + \", \" + str(correct))\n",
    "    \n",
    "    \n",
    "plot_confusion_matrix(confusion_matrix(Y_test, prediction), classes=[\"ants\",\"grapes\"],normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e3c19",
   "metadata": {},
   "source": [
    "## Bagging for Decision tree\n",
    "\n",
    "Let us see what bagging would result in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "plt.close(\"all\")\n",
    "clf = BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(),\n",
    "                       max_samples=0.7, random_state=0).fit(X_train, Y_train)\n",
    "tree_model = clf.estimators_[0]\n",
    "for estimator in clf.estimators_:\n",
    "    if estimator.score(X_test, Y_test) > tree_model.score(X_test, Y_test):\n",
    "        tree_model = estimator\n",
    "# We plot the tree with the best testing score for illustration purpose\n",
    "\"\"\"dot_data = tree.export_graphviz(tree_model, out_file=None, \n",
    "                         feature_names=[\"pixel number \"+str(k) for k in range(784)],  \n",
    "                         class_names=[\"ANT\", \"GRAPES\"],  \n",
    "                         filled=True, rounded=True)  \n",
    "graphv = graphviz.Source(dot_data)\n",
    "graphv\"\"\"\n",
    "\n",
    "plt.figure(figsize=(250,100))\n",
    "a = tree.plot_tree(tree_model,\n",
    "               feature_names = [\"pixel number \"+str(k) for k in range(784)], \n",
    "               class_names=[\"ANT\", \"GRAPES\"],\n",
    "               filled = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98085e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score\", clf.score(X_train, Y_train))\n",
    "print(\"test score\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c410aae",
   "metadata": {},
   "source": [
    "We can see that we slightly improved the performance of our classifier. However, if we impose the maximum depth found with the cross-validation previously, we obtain the following result, which is slightly better :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "plt.close(\"all\")\n",
    "clf = BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=4),\n",
    "                       max_samples=0.7, random_state=0).fit(X_train, Y_train)\n",
    "\n",
    "print(\"training score\", clf.score(X_train, Y_train))\n",
    "print(\"test score\", clf.score(X_test, Y_test))\n",
    "bagging_score = clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5010f",
   "metadata": {},
   "source": [
    "Hence we have slighlty improved the quality of our classifier using Bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1048e",
   "metadata": {},
   "source": [
    "And we can see the prediction using this method for 20 drawings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "prediction = clf.predict(X_test)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5, 5, i)\n",
    "    tpm = np.random.randint(0,len(X_test))\n",
    "    plt.imshow(X_test[tpm].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "    # Prediction\n",
    "    correct = True\n",
    "    if prediction[tpm] != Y_test[tpm]:\n",
    "        correct = False\n",
    "    if prediction[tpm]==0:\n",
    "        label_predicted = \"ant\"\n",
    "    else:\n",
    "        label_predicted = \"grape\"\n",
    "    plt.title(\"Prediction : \" + label_predicted + \", \" + str(correct))\n",
    "    \n",
    "plot_confusion_matrix(confusion_matrix(Y_test, prediction), classes=[\"ants\",\"grapes\"],normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a59af0b",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Here, bootstrap samples are used when building trees as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3db690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "plt.close(\"all\")\n",
    "clf = RandomForestClassifier(n_jobs=2, max_samples=0.7).fit(X_train, Y_train)\n",
    "tree_model = clf.estimators_[0]\n",
    "for estimator in clf.estimators_:\n",
    "    if estimator.score(X_test, Y_test) > tree_model.score(X_test, Y_test):\n",
    "        tree_model = estimator\n",
    "        \n",
    "# We plot the tree with the best testing score for illustrating purpose\n",
    "\"\"\"dot_data = tree.export_graphviz(tree_model, out_file=None, \n",
    "                         feature_names=[\"pixel number \"+str(k) for k in range(784)],  \n",
    "                         class_names=[\"ANT\", \"GRAPES\"],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graphv = graphviz.Source(dot_data)\n",
    "graphv\"\"\"\n",
    "plt.figure(figsize=(250,100))\n",
    "a = tree.plot_tree(tree_model,\n",
    "               feature_names = [\"pixel number \"+str(k) for k in range(784)], \n",
    "               class_names=[\"ANT\", \"GRAPES\"],\n",
    "               filled = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11685e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score\", clf.score(X_train, Y_train))\n",
    "print(\"test score\", clf.score(X_test, Y_test))\n",
    "random_forest_score = clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573dea0",
   "metadata": {},
   "source": [
    "This classifier is a slightly better improvement than the previous decision tree classifiers, but slightly less good than the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "prediction = clf.predict(X_test)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5, 5, i)\n",
    "    tpm = np.random.randint(0,len(X_test))\n",
    "    plt.imshow(X_test[tpm].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "    # Prediction\n",
    "    correct = True\n",
    "    if prediction[tpm] != Y_test[tpm]:\n",
    "        correct = False\n",
    "    if prediction[tpm]==0:\n",
    "        label_predicted = \"ant\"\n",
    "    else:\n",
    "        label_predicted = \"grape\"\n",
    "    plt.title(\"Prediction : \" + label_predicted + \", \" + str(correct))\n",
    "    \n",
    "plot_confusion_matrix(confusion_matrix(Y_test, prediction), classes=[\"ants\",\"grapes\"],normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c917f",
   "metadata": {},
   "source": [
    "## AdaBoost \n",
    "Not necessary since we do not have a weak classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597c0e5",
   "metadata": {},
   "source": [
    "## Neuronal Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"Using tensorflow version \" + str(tf.__version__))\n",
    "print(\"Using keras version \" + str(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d31f4b",
   "metadata": {},
   "source": [
    "We turn train and test labels to one-hot encoding using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25, random_state=69)\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7860fa",
   "metadata": {},
   "source": [
    "We cast the pixels to floats, and normalize the images so that they have zero-mean and unitary deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7518f44",
   "metadata": {},
   "source": [
    "We will create a fully connected network.\n",
    "For the fully connected layer, we are using this architecture: \n",
    "$$ (784) \\rightarrow (300) \\rightarrow (2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "output_shape = 300\n",
    "model = Sequential([\n",
    "   Dense(output_shape, activation='sigmoid', input_shape=(input_shape,)),\n",
    "    Flatten(),\n",
    "   Dense(2, activation='softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa5543",
   "metadata": {},
   "source": [
    "Then we instantiate a SGD optimizer with a tentative learning rate of $\\eta = 10^{-2}$ and compile the model using the ```'categorical_crossentropy'``` loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "\n",
    "model.compile(\n",
    "   optimizer='sgd',\n",
    "   loss='categorical_crossentropy',\n",
    "   metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# We can now have a look at the defined model topology\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455aff5",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "First, in order to observe the behaviour, we train the model for 15 epochs using the ```.fit()``` method, validating the model at each epoch and keeping track of the training history for later plotting. We enable ```.fit()``` verbose mode in order to visualize the training.\n",
    "\n",
    "In order to accelerate training, we use the ```batch_size``` option of ```.fit()```, which will process a batch of examples at the same time, and make one update for all of them, averaged over the gradients for each training example of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb84943",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "   x=X_train,\n",
    "   y=Y_train,\n",
    "   epochs=epochs,\n",
    "   batch_size=64,\n",
    "    verbose = 1, \n",
    "   validation_data = (X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407b364",
   "metadata": {},
   "source": [
    "### Visualizing the network performance\n",
    "\n",
    "We visualize the training history using the ```pyplot``` package:\n",
    "- In one graph, we plot the train and vaidation loss functions,\n",
    "- In another graph, we plot the train and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73902bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "_, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# summarize history for loss\n",
    "axes[0].plot(history.history['loss'])\n",
    "axes[0].plot(history.history['val_loss'])\n",
    "axes[0].set_title('Train and validation losses')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('loss')\n",
    "axes[0].legend(['Train loss', 'Validation loss'], loc='best')\n",
    "\n",
    "# summarize history for accuracy\n",
    "axes[1].plot(history.history['accuracy'])\n",
    "axes[1].plot(history.history['val_accuracy'])\n",
    "axes[1].set_title('Train and validation accuracies')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('accuracy')\n",
    "axes[1].legend(['Train accuracy', 'Validation accuracy'], loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1855b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "epochs_optim = 2\n",
    "batch_optim = 32\n",
    "for epochs in range(2,21):\n",
    "    for batch in [32*k for k in range(1,4)]:\n",
    "        history = model.fit(\n",
    "       x=X_train,\n",
    "        y=Y_train,\n",
    "       epochs=epochs,\n",
    "       batch_size=batch,\n",
    "        verbose = 0, \n",
    "       validation_data = (X_test, Y_test))\n",
    "        if model.evaluate(X_test, Y_test, verbose=0)[1] > accuracy :\n",
    "            accuracy = model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "            epochs_optim = epochs\n",
    "            batch_optim = batch\n",
    "print(accuracy)\n",
    "print(epochs_optim)\n",
    "print(batch_optim)\n",
    "NN_sigmoid_simple_score = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d998477",
   "metadata": {},
   "source": [
    "So it seems that 2 epochs and a batch_size of 96 are good hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64170fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "model.fit(\n",
    "   x=X_train,\n",
    "   y=Y_train,\n",
    "   epochs=2,\n",
    "   batch_size=96,\n",
    "    verbose = 1, \n",
    "   validation_data = (X_test, Y_test)\n",
    ")\n",
    "prediction = model.predict(\n",
    "    x=X_test,\n",
    "    batch_size=96,\n",
    "    verbose=0\n",
    ")\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5, 5, i)\n",
    "    tpm = np.random.randint(0,len(X_test))\n",
    "    plt.imshow(X_test[tpm].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "    # Prediction\n",
    "    correct = True\n",
    "    if np.argmax(prediction[tpm]) != np.argmax(Y_test[tpm]):\n",
    "        correct = False\n",
    "    if np.argmax(prediction[tpm])==0:\n",
    "        label_predicted = \"ant\"\n",
    "    else:\n",
    "        label_predicted = \"grape\"\n",
    "    plt.title(\"Prediction : \" + label_predicted + \", \" + str(correct))\n",
    "print(model.evaluate(X_test, Y_test))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(Y_test, prediction), classes=[\"ants\",\"grapes\"],normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239c87a",
   "metadata": {},
   "source": [
    "### Replacing activation function by ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69caf0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "model = Sequential([\n",
    "   Dense(output_shape, activation='relu', input_shape=(input_shape,)),\n",
    "    Flatten(),\n",
    "   Dense(2, activation='softmax')])\n",
    "model.compile(\n",
    "   optimizer='sgd',\n",
    "   loss='categorical_crossentropy',\n",
    "   metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "   x=X_train,\n",
    "   y=Y_train,\n",
    "   epochs=epochs,\n",
    "   batch_size=64,\n",
    "    verbose = 0, \n",
    "   validation_data = (X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc85c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "_, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# summarize history for loss\n",
    "axes[0].plot(history.history['loss'])\n",
    "axes[0].plot(history.history['val_loss'])\n",
    "axes[0].set_title('Train and validation losses')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('loss')\n",
    "axes[0].legend(['Train loss', 'Validation loss'], loc='best')\n",
    "\n",
    "# summarize history for accuracy\n",
    "axes[1].plot(history.history['accuracy'])\n",
    "axes[1].plot(history.history['val_accuracy'])\n",
    "axes[1].set_title('Train and validation accuracies')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('accuracy')\n",
    "axes[1].legend(['Train accuracy', 'Validation accuracy'], loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf21d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7288ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "epochs_optim = 2\n",
    "batch_optim = 32\n",
    "for epochs in range(2,21):\n",
    "    for batch in [32*k for k in range(1,4)]:\n",
    "        history = model.fit(\n",
    "       x=X_train,\n",
    "        y=Y_train,\n",
    "       epochs=epochs,\n",
    "       batch_size=batch,\n",
    "        verbose = 0, \n",
    "       validation_data = (X_test, Y_test))\n",
    "        if model.evaluate(X_test, Y_test, verbose=0)[1] > accuracy :\n",
    "            accuracy = model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "            epochs_optim = epochs\n",
    "            batch_optim = batch\n",
    "print(accuracy)\n",
    "print(epochs_optim)\n",
    "print(batch_optim)\n",
    "NN_relu_simple = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "model.fit(\n",
    "   x=X_train,\n",
    "   y=Y_train,\n",
    "   epochs=2,\n",
    "   batch_size=32,\n",
    "    verbose = 0, \n",
    "   validation_data = (X_test, Y_test)\n",
    ")\n",
    "prediction = model.predict(\n",
    "    x=X_test,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5, 5, i)\n",
    "    tpm = np.random.randint(0,len(X_test))\n",
    "    plt.imshow(X_test[tpm].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "    # Prediction\n",
    "    correct = True\n",
    "    if np.argmax(prediction[tpm]) != np.argmax(Y_test[tpm]):\n",
    "        correct = False\n",
    "    if np.argmax(prediction[tpm])==0:\n",
    "        label_predicted = \"ant\"\n",
    "    else:\n",
    "        label_predicted = \"grape\"\n",
    "    plt.title(\"Prediction : \" + label_predicted + \", \" + str(correct))\n",
    "print(model.evaluate(X_test, Y_test))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(Y_test, prediction), classes=[\"ants\",\"grapes\"],normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50458e",
   "metadata": {},
   "source": [
    "### With a larger architecture\n",
    "\n",
    "$$ (784) \\rightarrow (300) \\rightarrow (128) \\rightarrow (84) \\rightarrow (2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "model = Sequential([\n",
    "   Dense(output_shape, activation='sigmoid', input_shape=(input_shape,)),\n",
    "    Flatten(),\n",
    "   Dense(128, activation='softmax'),\n",
    "    Flatten(),\n",
    "    Dense(84, activation='softmax'),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "model.compile(\n",
    "   optimizer='sgd',\n",
    "   loss='categorical_crossentropy',\n",
    "   metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "   x=X_train,\n",
    "   y=Y_train,\n",
    "   epochs=epochs,\n",
    "   batch_size=64,\n",
    "    verbose = 0, \n",
    "   validation_data = (X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "_, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# summarize history for loss\n",
    "axes[0].plot(history.history['loss'])\n",
    "axes[0].plot(history.history['val_loss'])\n",
    "axes[0].set_title('Train and validation losses')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('loss')\n",
    "axes[0].legend(['Train loss', 'Validation loss'], loc='best')\n",
    "\n",
    "# summarize history for accuracy\n",
    "axes[1].plot(history.history['accuracy'])\n",
    "axes[1].plot(history.history['val_accuracy'])\n",
    "axes[1].set_title('Train and validation accuracies')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('accuracy')\n",
    "axes[1].legend(['Train accuracy', 'Validation accuracy'], loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "epochs_optim = 2\n",
    "batch_optim = 32\n",
    "for epochs in range(2,21):\n",
    "    for batch in [32*k for k in range(1,4)]:\n",
    "        history = model.fit(\n",
    "   x=X_train,\n",
    "   y=Y_train,\n",
    "   epochs=epochs,\n",
    "   batch_size=batch,\n",
    "    verbose = 0, \n",
    "   validation_data = (X_test, Y_test))\n",
    "        if model.evaluate(X_test, Y_test, verbose=0)[1] > accuracy :\n",
    "            accuracy = model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "            epochs_optim = epochs\n",
    "            batch_optim = batch\n",
    "print(accuracy)\n",
    "print(epochs_optim)\n",
    "print(batch_optim)\n",
    "NN_large = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "model.fit(\n",
    "   x=X_train,\n",
    "   y=Y_train,\n",
    "   epochs=2,\n",
    "   batch_size=32,\n",
    "    verbose = 0, \n",
    "   validation_data = (X_test, Y_test)\n",
    ")\n",
    "prediction = model.predict(\n",
    "    x=X_test,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5, 5, i)\n",
    "    tpm = np.random.randint(0,len(X_test))\n",
    "    plt.imshow(X_test[tpm].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "    # Prediction\n",
    "    correct = True\n",
    "    if np.argmax(prediction[tpm]) != np.argmax(Y_test[tpm]):\n",
    "        correct = False\n",
    "    if np.argmax(prediction[tpm])==0:\n",
    "        label_predicted = \"ant\"\n",
    "    else:\n",
    "        label_predicted = \"grape\"\n",
    "    plt.title(\"Prediction : \" + label_predicted + \", \" + str(correct))\n",
    "plt.show()\n",
    "print(model.evaluate(X_test, Y_test))\n",
    "\n",
    "\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(Y_test, prediction), classes=[\"ants\",\"grapes\"],normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb74c3c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Let us sum up the performance of all the previous classifiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f21866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "plt.bar([\"SVM\", \"Decision tree\", \"Bagging\", \"Random Forest\", \"SImple NN with sigmoid\", \"Simple NN with relu\", \"More complex NN\"], [svm_accuracy, simple_decision_tree_score, bagging_score, random_forest_score, NN_sigmoid_simple_score, NN_relu_simple, NN_large])\n",
    "plt.xticks(range(7), [\"SVM\", \"Decision tree\", \"Bagging\", \"Random Forest\", \"SImple NN with sigmoid\", \"Simple NN with relu\", \"More complex NN\"], rotation=60)\n",
    "plt.ylabel(\"Testing score\")\n",
    "plt.title(\"Performance of the different classifiers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163adcd7",
   "metadata": {},
   "source": [
    "It appears that the classifier given by the SVM is the best one. However, the classifiers given by the random forest or the neural network for a sigmoid activation function and a small architecture are also good. In conclusion, we came up with three good classifiers that have a good performance for this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
